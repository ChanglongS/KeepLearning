{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:56:34.959260Z",
     "start_time": "2024-12-01T12:56:34.231068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris_dict_data = load_iris()\n",
    "print(type(iris_dict_data))\n",
    "print(iris_dict_data)\n",
    "print(iris_dict_data.keys())\n",
    "print('-' * 80)\n",
    "print(iris_dict_data.feature_names)\n",
    "print('*' * 80)\n",
    "print(iris_dict_data.target_names)\n",
    "print('-.' * 40)\n",
    "print(iris_dict_data.DESCR)\n",
    "print('-' * 80)\n",
    "print(iris_dict_data.target)\n",
    "print(iris_dict_data.data)\n",
    "print(iris_dict_data.data.shape)"
   ],
   "id": "f7c36760309c265",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils._bunch.Bunch'>\n",
      "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
      "       [4.9, 3. , 1.4, 0.2],\n",
      "       [4.7, 3.2, 1.3, 0.2],\n",
      "       [4.6, 3.1, 1.5, 0.2],\n",
      "       [5. , 3.6, 1.4, 0.2],\n",
      "       [5.4, 3.9, 1.7, 0.4],\n",
      "       [4.6, 3.4, 1.4, 0.3],\n",
      "       [5. , 3.4, 1.5, 0.2],\n",
      "       [4.4, 2.9, 1.4, 0.2],\n",
      "       [4.9, 3.1, 1.5, 0.1],\n",
      "       [5.4, 3.7, 1.5, 0.2],\n",
      "       [4.8, 3.4, 1.6, 0.2],\n",
      "       [4.8, 3. , 1.4, 0.1],\n",
      "       [4.3, 3. , 1.1, 0.1],\n",
      "       [5.8, 4. , 1.2, 0.2],\n",
      "       [5.7, 4.4, 1.5, 0.4],\n",
      "       [5.4, 3.9, 1.3, 0.4],\n",
      "       [5.1, 3.5, 1.4, 0.3],\n",
      "       [5.7, 3.8, 1.7, 0.3],\n",
      "       [5.1, 3.8, 1.5, 0.3],\n",
      "       [5.4, 3.4, 1.7, 0.2],\n",
      "       [5.1, 3.7, 1.5, 0.4],\n",
      "       [4.6, 3.6, 1. , 0.2],\n",
      "       [5.1, 3.3, 1.7, 0.5],\n",
      "       [4.8, 3.4, 1.9, 0.2],\n",
      "       [5. , 3. , 1.6, 0.2],\n",
      "       [5. , 3.4, 1.6, 0.4],\n",
      "       [5.2, 3.5, 1.5, 0.2],\n",
      "       [5.2, 3.4, 1.4, 0.2],\n",
      "       [4.7, 3.2, 1.6, 0.2],\n",
      "       [4.8, 3.1, 1.6, 0.2],\n",
      "       [5.4, 3.4, 1.5, 0.4],\n",
      "       [5.2, 4.1, 1.5, 0.1],\n",
      "       [5.5, 4.2, 1.4, 0.2],\n",
      "       [4.9, 3.1, 1.5, 0.2],\n",
      "       [5. , 3.2, 1.2, 0.2],\n",
      "       [5.5, 3.5, 1.3, 0.2],\n",
      "       [4.9, 3.6, 1.4, 0.1],\n",
      "       [4.4, 3. , 1.3, 0.2],\n",
      "       [5.1, 3.4, 1.5, 0.2],\n",
      "       [5. , 3.5, 1.3, 0.3],\n",
      "       [4.5, 2.3, 1.3, 0.3],\n",
      "       [4.4, 3.2, 1.3, 0.2],\n",
      "       [5. , 3.5, 1.6, 0.6],\n",
      "       [5.1, 3.8, 1.9, 0.4],\n",
      "       [4.8, 3. , 1.4, 0.3],\n",
      "       [5.1, 3.8, 1.6, 0.2],\n",
      "       [4.6, 3.2, 1.4, 0.2],\n",
      "       [5.3, 3.7, 1.5, 0.2],\n",
      "       [5. , 3.3, 1.4, 0.2],\n",
      "       [7. , 3.2, 4.7, 1.4],\n",
      "       [6.4, 3.2, 4.5, 1.5],\n",
      "       [6.9, 3.1, 4.9, 1.5],\n",
      "       [5.5, 2.3, 4. , 1.3],\n",
      "       [6.5, 2.8, 4.6, 1.5],\n",
      "       [5.7, 2.8, 4.5, 1.3],\n",
      "       [6.3, 3.3, 4.7, 1.6],\n",
      "       [4.9, 2.4, 3.3, 1. ],\n",
      "       [6.6, 2.9, 4.6, 1.3],\n",
      "       [5.2, 2.7, 3.9, 1.4],\n",
      "       [5. , 2. , 3.5, 1. ],\n",
      "       [5.9, 3. , 4.2, 1.5],\n",
      "       [6. , 2.2, 4. , 1. ],\n",
      "       [6.1, 2.9, 4.7, 1.4],\n",
      "       [5.6, 2.9, 3.6, 1.3],\n",
      "       [6.7, 3.1, 4.4, 1.4],\n",
      "       [5.6, 3. , 4.5, 1.5],\n",
      "       [5.8, 2.7, 4.1, 1. ],\n",
      "       [6.2, 2.2, 4.5, 1.5],\n",
      "       [5.6, 2.5, 3.9, 1.1],\n",
      "       [5.9, 3.2, 4.8, 1.8],\n",
      "       [6.1, 2.8, 4. , 1.3],\n",
      "       [6.3, 2.5, 4.9, 1.5],\n",
      "       [6.1, 2.8, 4.7, 1.2],\n",
      "       [6.4, 2.9, 4.3, 1.3],\n",
      "       [6.6, 3. , 4.4, 1.4],\n",
      "       [6.8, 2.8, 4.8, 1.4],\n",
      "       [6.7, 3. , 5. , 1.7],\n",
      "       [6. , 2.9, 4.5, 1.5],\n",
      "       [5.7, 2.6, 3.5, 1. ],\n",
      "       [5.5, 2.4, 3.8, 1.1],\n",
      "       [5.5, 2.4, 3.7, 1. ],\n",
      "       [5.8, 2.7, 3.9, 1.2],\n",
      "       [6. , 2.7, 5.1, 1.6],\n",
      "       [5.4, 3. , 4.5, 1.5],\n",
      "       [6. , 3.4, 4.5, 1.6],\n",
      "       [6.7, 3.1, 4.7, 1.5],\n",
      "       [6.3, 2.3, 4.4, 1.3],\n",
      "       [5.6, 3. , 4.1, 1.3],\n",
      "       [5.5, 2.5, 4. , 1.3],\n",
      "       [5.5, 2.6, 4.4, 1.2],\n",
      "       [6.1, 3. , 4.6, 1.4],\n",
      "       [5.8, 2.6, 4. , 1.2],\n",
      "       [5. , 2.3, 3.3, 1. ],\n",
      "       [5.6, 2.7, 4.2, 1.3],\n",
      "       [5.7, 3. , 4.2, 1.2],\n",
      "       [5.7, 2.9, 4.2, 1.3],\n",
      "       [6.2, 2.9, 4.3, 1.3],\n",
      "       [5.1, 2.5, 3. , 1.1],\n",
      "       [5.7, 2.8, 4.1, 1.3],\n",
      "       [6.3, 3.3, 6. , 2.5],\n",
      "       [5.8, 2.7, 5.1, 1.9],\n",
      "       [7.1, 3. , 5.9, 2.1],\n",
      "       [6.3, 2.9, 5.6, 1.8],\n",
      "       [6.5, 3. , 5.8, 2.2],\n",
      "       [7.6, 3. , 6.6, 2.1],\n",
      "       [4.9, 2.5, 4.5, 1.7],\n",
      "       [7.3, 2.9, 6.3, 1.8],\n",
      "       [6.7, 2.5, 5.8, 1.8],\n",
      "       [7.2, 3.6, 6.1, 2.5],\n",
      "       [6.5, 3.2, 5.1, 2. ],\n",
      "       [6.4, 2.7, 5.3, 1.9],\n",
      "       [6.8, 3. , 5.5, 2.1],\n",
      "       [5.7, 2.5, 5. , 2. ],\n",
      "       [5.8, 2.8, 5.1, 2.4],\n",
      "       [6.4, 3.2, 5.3, 2.3],\n",
      "       [6.5, 3. , 5.5, 1.8],\n",
      "       [7.7, 3.8, 6.7, 2.2],\n",
      "       [7.7, 2.6, 6.9, 2.3],\n",
      "       [6. , 2.2, 5. , 1.5],\n",
      "       [6.9, 3.2, 5.7, 2.3],\n",
      "       [5.6, 2.8, 4.9, 2. ],\n",
      "       [7.7, 2.8, 6.7, 2. ],\n",
      "       [6.3, 2.7, 4.9, 1.8],\n",
      "       [6.7, 3.3, 5.7, 2.1],\n",
      "       [7.2, 3.2, 6. , 1.8],\n",
      "       [6.2, 2.8, 4.8, 1.8],\n",
      "       [6.1, 3. , 4.9, 1.8],\n",
      "       [6.4, 2.8, 5.6, 2.1],\n",
      "       [7.2, 3. , 5.8, 1.6],\n",
      "       [7.4, 2.8, 6.1, 1.9],\n",
      "       [7.9, 3.8, 6.4, 2. ],\n",
      "       [6.4, 2.8, 5.6, 2.2],\n",
      "       [6.3, 2.8, 5.1, 1.5],\n",
      "       [6.1, 2.6, 5.6, 1.4],\n",
      "       [7.7, 3. , 6.1, 2.3],\n",
      "       [6.3, 3.4, 5.6, 2.4],\n",
      "       [6.4, 3.1, 5.5, 1.8],\n",
      "       [6. , 3. , 4.8, 1.8],\n",
      "       [6.9, 3.1, 5.4, 2.1],\n",
      "       [6.7, 3.1, 5.6, 2.4],\n",
      "       [6.9, 3.1, 5.1, 2.3],\n",
      "       [5.8, 2.7, 5.1, 1.9],\n",
      "       [6.8, 3.2, 5.9, 2.3],\n",
      "       [6.7, 3.3, 5.7, 2.5],\n",
      "       [6.7, 3. , 5.2, 2.3],\n",
      "       [6.3, 2.5, 5. , 1.9],\n",
      "       [6.5, 3. , 5.2, 2. ],\n",
      "       [6.2, 3.4, 5.4, 2.3],\n",
      "       [5.9, 3. , 5.1, 1.8]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), 'frame': None, 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'), 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 150 (50 in each of three classes)\\n:Number of Attributes: 4 numeric, predictive attributes and the class\\n:Attribute Information:\\n    - sepal length in cm\\n    - sepal width in cm\\n    - petal length in cm\\n    - petal width in cm\\n    - class:\\n            - Iris-Setosa\\n            - Iris-Versicolour\\n            - Iris-Virginica\\n\\n:Summary Statistics:\\n\\n============== ==== ==== ======= ===== ====================\\n                Min  Max   Mean    SD   Class Correlation\\n============== ==== ==== ======= ===== ====================\\nsepal length:   4.3  7.9   5.84   0.83    0.7826\\nsepal width:    2.0  4.4   3.05   0.43   -0.4194\\npetal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\npetal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n============== ==== ==== ======= ===== ====================\\n\\n:Missing Attribute Values: None\\n:Class Distribution: 33.3% for each of 3 classes.\\n:Creator: R.A. Fisher\\n:Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n:Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. dropdown:: References\\n\\n  - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n    Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n    Mathematical Statistics\" (John Wiley, NY, 1950).\\n  - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n    (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n  - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n    Structure and Classification Rule for Recognition in Partially Exposed\\n    Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n    Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n  - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n    on Information Theory, May 1972, 431-433.\\n  - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n    conceptual clustering system finds 3 classes in the data.\\n  - Many, many more ...\\n', 'feature_names': ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'], 'filename': 'iris.csv', 'data_module': 'sklearn.datasets.data'}\n",
      "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n",
      "--------------------------------------------------------------------------------\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "********************************************************************************\n",
      "['setosa' 'versicolor' 'virginica']\n",
      "-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.\n",
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 150 (50 in each of three classes)\n",
      ":Number of Attributes: 4 numeric, predictive attributes and the class\n",
      ":Attribute Information:\n",
      "    - sepal length in cm\n",
      "    - sepal width in cm\n",
      "    - petal length in cm\n",
      "    - petal width in cm\n",
      "    - class:\n",
      "            - Iris-Setosa\n",
      "            - Iris-Versicolour\n",
      "            - Iris-Virginica\n",
      "\n",
      ":Summary Statistics:\n",
      "\n",
      "============== ==== ==== ======= ===== ====================\n",
      "                Min  Max   Mean    SD   Class Correlation\n",
      "============== ==== ==== ======= ===== ====================\n",
      "sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "============== ==== ==== ======= ===== ====================\n",
      "\n",
      ":Missing Attribute Values: None\n",
      ":Class Distribution: 33.3% for each of 3 classes.\n",
      ":Creator: R.A. Fisher\n",
      ":Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      ":Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. dropdown:: References\n",
      "\n",
      "  - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "    Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "    Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "  - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "    (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "  - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "    Structure and Classification Rule for Recognition in Partially Exposed\n",
      "    Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "    Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "  - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "    on Information Theory, May 1972, 431-433.\n",
      "  - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "    conceptual clustering system finds 3 classes in the data.\n",
      "  - Many, many more ...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "(150, 4)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:56:34.993446Z",
     "start_time": "2024-12-01T12:56:34.962293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(iris_dict_data.data, iris_dict_data.target, test_size=0.25,\n",
    "                                                    random_state=1)\n",
    "print('x_train, y_train: ', x_train, y_train)\n",
    "print('x_train shape: ', x_train.shape)\n",
    "print('x_test, y_test: ', x_test, y_test)\n",
    "print('x_test shape: ', x_test.shape)"
   ],
   "id": "af14c8a263c14ba7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train, y_train:  [[6.5 2.8 4.6 1.5]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.  3.  4.8 1.8]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [4.9 3.6 1.4 0.1]] [1 2 2 0 2 2 1 2 0 0 0 1 0 0 2 2 2 2 2 1 2 1 0 2 2 0 0 2 0 2 2 1 1 2 2 0 1\n",
      " 1 2 1 2 1 0 0 0 2 0 1 2 2 0 0 1 0 2 1 2 2 1 2 2 1 0 1 0 1 1 0 1 0 0 2 2 2\n",
      " 0 0 1 0 2 0 2 2 0 2 0 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 2 0 0 2 1 2 1 2 2 1 2\n",
      " 0]\n",
      "x_train shape:  (112, 4)\n",
      "x_test, y_test:  [[5.8 4.  1.2 0.2]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.7 3.  5.  1.7]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [5.2 3.4 1.4 0.2]] [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1\n",
      " 0]\n",
      "x_test shape:  (38, 4)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:56:35.271265Z",
     "start_time": "2024-12-01T12:56:35.121194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "news = fetch_20newsgroups(subset='all', data_home='data')\n",
    "print(news.DESCR)\n",
    "print('-' * 81)\n",
    "print(news.keys())\n",
    "print('-' * 80)\n",
    "print(news.data[0])\n",
    "print('-' * 80)\n",
    "print(news.target_names)"
   ],
   "id": "4c6dfc73e4ab81b1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _20newsgroups_dataset:\n",
      "\n",
      "The 20 newsgroups text dataset\n",
      "------------------------------\n",
      "\n",
      "The 20 newsgroups dataset comprises around 18000 newsgroups posts on\n",
      "20 topics split in two subsets: one for training (or development)\n",
      "and the other one for testing (or for performance evaluation). The split\n",
      "between the train and test set is based upon a messages posted before\n",
      "and after a specific date.\n",
      "\n",
      "This module contains two loaders. The first one,\n",
      ":func:`sklearn.datasets.fetch_20newsgroups`,\n",
      "returns a list of the raw texts that can be fed to text feature\n",
      "extractors such as :class:`~sklearn.feature_extraction.text.CountVectorizer`\n",
      "with custom parameters so as to extract feature vectors.\n",
      "The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\n",
      "returns ready-to-use features, i.e., it is not necessary to use a feature\n",
      "extractor.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "=================   ==========\n",
      "Classes                     20\n",
      "Samples total            18846\n",
      "Dimensionality               1\n",
      "Features                  text\n",
      "=================   ==========\n",
      "\n",
      ".. dropdown:: Usage\n",
      "\n",
      "  The :func:`sklearn.datasets.fetch_20newsgroups` function is a data\n",
      "  fetching / caching functions that downloads the data archive from\n",
      "  the original `20 newsgroups website <http://people.csail.mit.edu/jrennie/20Newsgroups/>`__,\n",
      "  extracts the archive contents\n",
      "  in the ``~/scikit_learn_data/20news_home`` folder and calls the\n",
      "  :func:`sklearn.datasets.load_files` on either the training or\n",
      "  testing set folder, or both of them::\n",
      "\n",
      "    >>> from sklearn.datasets import fetch_20newsgroups\n",
      "    >>> newsgroups_train = fetch_20newsgroups(subset='train')\n",
      "\n",
      "    >>> from pprint import pprint\n",
      "    >>> pprint(list(newsgroups_train.target_names))\n",
      "    ['alt.atheism',\n",
      "     'comp.graphics',\n",
      "     'comp.os.ms-windows.misc',\n",
      "     'comp.sys.ibm.pc.hardware',\n",
      "     'comp.sys.mac.hardware',\n",
      "     'comp.windows.x',\n",
      "     'misc.forsale',\n",
      "     'rec.autos',\n",
      "     'rec.motorcycles',\n",
      "     'rec.sport.baseball',\n",
      "     'rec.sport.hockey',\n",
      "     'sci.crypt',\n",
      "     'sci.electronics',\n",
      "     'sci.med',\n",
      "     'sci.space',\n",
      "     'soc.religion.christian',\n",
      "     'talk.politics.guns',\n",
      "     'talk.politics.mideast',\n",
      "     'talk.politics.misc',\n",
      "     'talk.religion.misc']\n",
      "\n",
      "  The real data lies in the ``filenames`` and ``target`` attributes. The target\n",
      "  attribute is the integer index of the category::\n",
      "\n",
      "    >>> newsgroups_train.filenames.shape\n",
      "    (11314,)\n",
      "    >>> newsgroups_train.target.shape\n",
      "    (11314,)\n",
      "    >>> newsgroups_train.target[:10]\n",
      "    array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])\n",
      "\n",
      "  It is possible to load only a sub-selection of the categories by passing the\n",
      "  list of the categories to load to the\n",
      "  :func:`sklearn.datasets.fetch_20newsgroups` function::\n",
      "\n",
      "    >>> cats = ['alt.atheism', 'sci.space']\n",
      "    >>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
      "\n",
      "    >>> list(newsgroups_train.target_names)\n",
      "    ['alt.atheism', 'sci.space']\n",
      "    >>> newsgroups_train.filenames.shape\n",
      "    (1073,)\n",
      "    >>> newsgroups_train.target.shape\n",
      "    (1073,)\n",
      "    >>> newsgroups_train.target[:10]\n",
      "    array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
      "\n",
      ".. dropdown:: Converting text to vectors\n",
      "\n",
      "  In order to feed predictive or clustering models with the text data,\n",
      "  one first need to turn the text into vectors of numerical values suitable\n",
      "  for statistical analysis. This can be achieved with the utilities of the\n",
      "  ``sklearn.feature_extraction.text`` as demonstrated in the following\n",
      "  example that extract `TF-IDF <https://en.wikipedia.org/wiki/Tf-idf>`__ vectors\n",
      "  of unigram tokens from a subset of 20news::\n",
      "\n",
      "    >>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "    >>> categories = ['alt.atheism', 'talk.religion.misc',\n",
      "    ...               'comp.graphics', 'sci.space']\n",
      "    >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "    ...                                       categories=categories)\n",
      "    >>> vectorizer = TfidfVectorizer()\n",
      "    >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "    >>> vectors.shape\n",
      "    (2034, 34118)\n",
      "\n",
      "  The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero\n",
      "  components by sample in a more than 30000-dimensional space\n",
      "  (less than .5% non-zero features)::\n",
      "\n",
      "    >>> vectors.nnz / float(vectors.shape[0])\n",
      "    159.01327...\n",
      "\n",
      "  :func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which\n",
      "  returns ready-to-use token counts features instead of file names.\n",
      "\n",
      ".. dropdown:: Filtering text for more realistic training\n",
      "\n",
      "  It is easy for a classifier to overfit on particular things that appear in the\n",
      "  20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very\n",
      "  high F-scores, but their results would not generalize to other documents that\n",
      "  aren't from this window of time.\n",
      "\n",
      "  For example, let's look at the results of a multinomial Naive Bayes classifier,\n",
      "  which is fast to train and achieves a decent F-score::\n",
      "\n",
      "    >>> from sklearn.naive_bayes import MultinomialNB\n",
      "    >>> from sklearn import metrics\n",
      "    >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "    ...                                      categories=categories)\n",
      "    >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "    >>> clf = MultinomialNB(alpha=.01)\n",
      "    >>> clf.fit(vectors, newsgroups_train.target)\n",
      "    MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "    >>> pred = clf.predict(vectors_test)\n",
      "    >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
      "    0.88213...\n",
      "\n",
      "  (The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles\n",
      "  the training and test data, instead of segmenting by time, and in that case\n",
      "  multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious\n",
      "  yet of what's going on inside this classifier?)\n",
      "\n",
      "  Let's take a look at what the most informative features are:\n",
      "\n",
      "    >>> import numpy as np\n",
      "    >>> def show_top10(classifier, vectorizer, categories):\n",
      "    ...     feature_names = vectorizer.get_feature_names_out()\n",
      "    ...     for i, category in enumerate(categories):\n",
      "    ...         top10 = np.argsort(classifier.coef_[i])[-10:]\n",
      "    ...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
      "    ...\n",
      "    >>> show_top10(clf, vectorizer, newsgroups_train.target_names)\n",
      "    alt.atheism: edu it and in you that is of to the\n",
      "    comp.graphics: edu in graphics it is for and of to the\n",
      "    sci.space: edu it that is in and space to of the\n",
      "    talk.religion.misc: not it you in is that and to of the\n",
      "\n",
      "\n",
      "  You can now see many things that these features have overfit to:\n",
      "\n",
      "  - Almost every group is distinguished by whether headers such as\n",
      "    ``NNTP-Posting-Host:`` and ``Distribution:`` appear more or less often.\n",
      "  - Another significant feature involves whether the sender is affiliated with\n",
      "    a university, as indicated either by their headers or their signature.\n",
      "  - The word \"article\" is a significant feature, based on how often people quote\n",
      "    previous posts like this: \"In article [article ID], [name] <[e-mail address]>\n",
      "    wrote:\"\n",
      "  - Other features match the names and e-mail addresses of particular people who\n",
      "    were posting at the time.\n",
      "\n",
      "  With such an abundance of clues that distinguish newsgroups, the classifiers\n",
      "  barely have to identify topics from text at all, and they all perform at the\n",
      "  same high level.\n",
      "\n",
      "  For this reason, the functions that load 20 Newsgroups data provide a\n",
      "  parameter called **remove**, telling it what kinds of information to strip out\n",
      "  of each file. **remove** should be a tuple containing any subset of\n",
      "  ``('headers', 'footers', 'quotes')``, telling it to remove headers, signature\n",
      "  blocks, and quotation blocks respectively.\n",
      "\n",
      "    >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "    ...                                      remove=('headers', 'footers', 'quotes'),\n",
      "    ...                                      categories=categories)\n",
      "    >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "    >>> pred = clf.predict(vectors_test)\n",
      "    >>> metrics.f1_score(pred, newsgroups_test.target, average='macro')\n",
      "    0.77310...\n",
      "\n",
      "  This classifier lost over a lot of its F-score, just because we removed\n",
      "  metadata that has little to do with topic classification.\n",
      "  It loses even more if we also strip this metadata from the training data:\n",
      "\n",
      "    >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "    ...                                       remove=('headers', 'footers', 'quotes'),\n",
      "    ...                                       categories=categories)\n",
      "    >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "    >>> clf = MultinomialNB(alpha=.01)\n",
      "    >>> clf.fit(vectors, newsgroups_train.target)\n",
      "    MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "    >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "    >>> pred = clf.predict(vectors_test)\n",
      "    >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
      "    0.76995...\n",
      "\n",
      "  Some other classifiers cope better with this harder version of the task. Try the\n",
      "  :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_text_feature_extraction.py`\n",
      "  example with and without the `remove` option to compare the results.\n",
      "\n",
      ".. topic:: Data Considerations\n",
      "\n",
      "  The Cleveland Indians is a major league baseball team based in Cleveland,\n",
      "  Ohio, USA. In December 2020, it was reported that \"After several months of\n",
      "  discussion sparked by the death of George Floyd and a national reckoning over\n",
      "  race and colonialism, the Cleveland Indians have decided to change their\n",
      "  name.\" Team owner Paul Dolan \"did make it clear that the team will not make\n",
      "  its informal nickname -- the Tribe -- its new team name.\" \"It's not going to\n",
      "  be a half-step away from the Indians,\" Dolan said.\"We will not have a Native\n",
      "  American-themed name.\"\n",
      "\n",
      "  https://www.mlb.com/news/cleveland-indians-team-name-change\n",
      "\n",
      ".. topic:: Recommendation\n",
      "\n",
      "  - When evaluating text classifiers on the 20 Newsgroups data, you\n",
      "    should strip newsgroup-related metadata. In scikit-learn, you can do this\n",
      "    by setting ``remove=('headers', 'footers', 'quotes')``. The F-score will be\n",
      "    lower because it is more realistic.\n",
      "  - This text dataset contains data which may be inappropriate for certain NLP\n",
      "    applications. An example is listed in the \"Data Considerations\" section\n",
      "    above. The challenge with using current text datasets in NLP for tasks such\n",
      "    as sentence completion, clustering, and other applications is that text\n",
      "    that is culturally biased and inflammatory will propagate biases. This\n",
      "    should be taken into consideration when using the dataset, reviewing the\n",
      "    output, and the bias should be documented.\n",
      "\n",
      ".. rubric:: Examples\n",
      "\n",
      "* :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_text_feature_extraction.py`\n",
      "* :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\n",
      "* :ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`\n",
      "* :ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n",
      "--------------------------------------------------------------------------------\n",
      "From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\n",
      "Subject: Pens fans reactions\n",
      "Organization: Post Office, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: po4.andrew.cmu.edu\n",
      "\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:56:35.283221Z",
     "start_time": "2024-12-01T12:56:35.275020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 接着来看回归的数据,是波士顿房价\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# 通过 fetch_openml 下载 Boston 数据集\n",
    "boston = fetch_openml(data_id=531, as_frame=True)\n",
    "print(boston.keys())\n",
    "print('-' * 80)\n",
    "print(boston.DESCR)\n",
    "print('-' * 80)\n",
    "print(boston.target)\n",
    "print('-' * 80)\n",
    "print(boston.target_names)\n",
    "print(boston.data.shape)\n",
    "print('-' * 80)\n",
    "print(boston.feature_names)"
   ],
   "id": "9d3d569e6a968682",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])\n",
      "--------------------------------------------------------------------------------\n",
      "**Author**:   \n",
      "**Source**: Unknown - Date unknown  \n",
      "**Please cite**:   \n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "Variables in order:\n",
      "CRIM     per capita crime rate by town\n",
      "ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "INDUS    proportion of non-retail business acres per town\n",
      "CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "NOX      nitric oxides concentration (parts per 10 million)\n",
      "RM       average number of rooms per dwelling\n",
      "AGE      proportion of owner-occupied units built prior to 1940\n",
      "DIS      weighted distances to five Boston employment centres\n",
      "RAD      index of accessibility to radial highways\n",
      "TAX      full-value property-tax rate per $10,000\n",
      "PTRATIO  pupil-teacher ratio by town\n",
      "B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "LSTAT    % lower status of the population\n",
      "MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "\n",
      "Information about the dataset\n",
      "CLASSTYPE: numeric\n",
      "CLASSINDEX: last\n",
      "\n",
      "Downloaded from openml.org.\n",
      "--------------------------------------------------------------------------------\n",
      "0      24.0\n",
      "1      21.6\n",
      "2      34.7\n",
      "3      33.4\n",
      "4      36.2\n",
      "       ... \n",
      "501    22.4\n",
      "502    20.6\n",
      "503    23.9\n",
      "504    22.0\n",
      "505    11.9\n",
      "Name: MEDV, Length: 506, dtype: float64\n",
      "--------------------------------------------------------------------------------\n",
      "['MEDV']\n",
      "(506, 13)\n",
      "--------------------------------------------------------------------------------\n",
      "['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:56:35.468390Z",
     "start_time": "2024-12-01T12:56:35.460535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "print(digits.data.shape)\n",
    "print(digits.feature_names)\n",
    "print('-' * 80)\n",
    "print(digits.target_names)\n",
    "print('=' * 80)\n",
    "print(digits.keys())\n",
    "print('-' * 80)\n",
    "print(digits.data)"
   ],
   "id": "2cd9d37127c8b3bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "['pixel_0_0', 'pixel_0_1', 'pixel_0_2', 'pixel_0_3', 'pixel_0_4', 'pixel_0_5', 'pixel_0_6', 'pixel_0_7', 'pixel_1_0', 'pixel_1_1', 'pixel_1_2', 'pixel_1_3', 'pixel_1_4', 'pixel_1_5', 'pixel_1_6', 'pixel_1_7', 'pixel_2_0', 'pixel_2_1', 'pixel_2_2', 'pixel_2_3', 'pixel_2_4', 'pixel_2_5', 'pixel_2_6', 'pixel_2_7', 'pixel_3_0', 'pixel_3_1', 'pixel_3_2', 'pixel_3_3', 'pixel_3_4', 'pixel_3_5', 'pixel_3_6', 'pixel_3_7', 'pixel_4_0', 'pixel_4_1', 'pixel_4_2', 'pixel_4_3', 'pixel_4_4', 'pixel_4_5', 'pixel_4_6', 'pixel_4_7', 'pixel_5_0', 'pixel_5_1', 'pixel_5_2', 'pixel_5_3', 'pixel_5_4', 'pixel_5_5', 'pixel_5_6', 'pixel_5_7', 'pixel_6_0', 'pixel_6_1', 'pixel_6_2', 'pixel_6_3', 'pixel_6_4', 'pixel_6_5', 'pixel_6_6', 'pixel_6_7', 'pixel_7_0', 'pixel_7_1', 'pixel_7_2', 'pixel_7_3', 'pixel_7_4', 'pixel_7_5', 'pixel_7_6', 'pixel_7_7']\n",
      "--------------------------------------------------------------------------------\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "================================================================================\n",
      "dict_keys(['data', 'target', 'frame', 'feature_names', 'target_names', 'images', 'DESCR'])\n",
      "--------------------------------------------------------------------------------\n",
      "[[ 0.  0.  5. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ... 10.  0.  0.]\n",
      " [ 0.  0.  0. ... 16.  9.  0.]\n",
      " ...\n",
      " [ 0.  0.  1. ...  6.  0.  0.]\n",
      " [ 0.  0.  2. ... 12.  0.  0.]\n",
      " [ 0.  0. 10. ... 12.  1.  0.]]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:56:35.643240Z",
     "start_time": "2024-12-01T12:56:35.637580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "data_dia = load_diabetes()\n",
    "print(data_dia.keys())\n",
    "print(data_dia.DESCR)\n",
    "print('-' * 80)\n",
    "print(data_dia.feature_names)\n",
    "print(data_dia.data.shape)\n",
    "print(data_dia.target)"
   ],
   "id": "75e8c088154a837f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'frame', 'DESCR', 'feature_names', 'data_filename', 'target_filename', 'data_module'])\n",
      ".. _diabetes_dataset:\n",
      "\n",
      "Diabetes dataset\n",
      "----------------\n",
      "\n",
      "Ten baseline variables, age, sex, body mass index, average blood\n",
      "pressure, and six blood serum measurements were obtained for each of n =\n",
      "442 diabetes patients, as well as the response of interest, a\n",
      "quantitative measure of disease progression one year after baseline.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 442\n",
      "\n",
      ":Number of Attributes: First 10 columns are numeric predictive values\n",
      "\n",
      ":Target: Column 11 is a quantitative measure of disease progression one year after baseline\n",
      "\n",
      ":Attribute Information:\n",
      "    - age     age in years\n",
      "    - sex\n",
      "    - bmi     body mass index\n",
      "    - bp      average blood pressure\n",
      "    - s1      tc, total serum cholesterol\n",
      "    - s2      ldl, low-density lipoproteins\n",
      "    - s3      hdl, high-density lipoproteins\n",
      "    - s4      tch, total cholesterol / HDL\n",
      "    - s5      ltg, possibly log of serum triglycerides level\n",
      "    - s6      glu, blood sugar level\n",
      "\n",
      "Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).\n",
      "\n",
      "Source URL:\n",
      "https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n",
      "\n",
      "For more information see:\n",
      "Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n",
      "(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n",
      "(442, 10)\n",
      "[151.  75. 141. 206. 135.  97. 138.  63. 110. 310. 101.  69. 179. 185.\n",
      " 118. 171. 166. 144.  97. 168.  68.  49.  68. 245. 184. 202. 137.  85.\n",
      " 131. 283. 129.  59. 341.  87.  65. 102. 265. 276. 252.  90. 100.  55.\n",
      "  61.  92. 259.  53. 190. 142.  75. 142. 155. 225.  59. 104. 182. 128.\n",
      "  52.  37. 170. 170.  61. 144.  52. 128.  71. 163. 150.  97. 160. 178.\n",
      "  48. 270. 202. 111.  85.  42. 170. 200. 252. 113. 143.  51.  52. 210.\n",
      "  65. 141.  55. 134.  42. 111.  98. 164.  48.  96.  90. 162. 150. 279.\n",
      "  92.  83. 128. 102. 302. 198.  95.  53. 134. 144. 232.  81. 104.  59.\n",
      " 246. 297. 258. 229. 275. 281. 179. 200. 200. 173. 180.  84. 121. 161.\n",
      "  99. 109. 115. 268. 274. 158. 107.  83. 103. 272.  85. 280. 336. 281.\n",
      " 118. 317. 235.  60. 174. 259. 178. 128.  96. 126. 288.  88. 292.  71.\n",
      " 197. 186.  25.  84.  96. 195.  53. 217. 172. 131. 214.  59.  70. 220.\n",
      " 268. 152.  47.  74. 295. 101. 151. 127. 237. 225.  81. 151. 107.  64.\n",
      " 138. 185. 265. 101. 137. 143. 141.  79. 292. 178.  91. 116.  86. 122.\n",
      "  72. 129. 142.  90. 158.  39. 196. 222. 277.  99. 196. 202. 155.  77.\n",
      " 191.  70.  73.  49.  65. 263. 248. 296. 214. 185.  78.  93. 252. 150.\n",
      "  77. 208.  77. 108. 160.  53. 220. 154. 259.  90. 246. 124.  67.  72.\n",
      " 257. 262. 275. 177.  71.  47. 187. 125.  78.  51. 258. 215. 303. 243.\n",
      "  91. 150. 310. 153. 346.  63.  89.  50.  39. 103. 308. 116. 145.  74.\n",
      "  45. 115. 264.  87. 202. 127. 182. 241.  66.  94. 283.  64. 102. 200.\n",
      " 265.  94. 230. 181. 156. 233.  60. 219.  80.  68. 332. 248.  84. 200.\n",
      "  55.  85.  89.  31. 129.  83. 275.  65. 198. 236. 253. 124.  44. 172.\n",
      " 114. 142. 109. 180. 144. 163. 147.  97. 220. 190. 109. 191. 122. 230.\n",
      " 242. 248. 249. 192. 131. 237.  78. 135. 244. 199. 270. 164.  72.  96.\n",
      " 306.  91. 214.  95. 216. 263. 178. 113. 200. 139. 139.  88. 148.  88.\n",
      " 243.  71.  77. 109. 272.  60.  54. 221.  90. 311. 281. 182. 321.  58.\n",
      " 262. 206. 233. 242. 123. 167.  63. 197.  71. 168. 140. 217. 121. 235.\n",
      " 245.  40.  52. 104. 132.  88.  69. 219.  72. 201. 110.  51. 277.  63.\n",
      " 118.  69. 273. 258.  43. 198. 242. 232. 175.  93. 168. 275. 293. 281.\n",
      "  72. 140. 189. 181. 209. 136. 261. 113. 131. 174. 257.  55.  84.  42.\n",
      " 146. 212. 233.  91. 111. 152. 120.  67. 310.  94. 183.  66. 173.  72.\n",
      "  49.  64.  48. 178. 104. 132. 220.  57.]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 训练集使用fit_transform, 测试集使用transform",
   "id": "88079c3233969bff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:56:46.171079Z",
     "start_time": "2024-12-01T12:56:35.816468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./data/FBlocation/train.csv')\n",
    "print(data.shape)\n",
    "print(data.columns)\n",
    "print(data.head(10))\n",
    "print('-' * 80)\n",
    "print(max(data['x']), min(data['x']))\n",
    "print(max(data['y']), min(data['y']))\n",
    "print(type(data))"
   ],
   "id": "1971ea69d996348a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29118021, 6)\n",
      "Index(['row_id', 'x', 'y', 'accuracy', 'time', 'place_id'], dtype='object')\n",
      "   row_id       x       y  accuracy    time    place_id\n",
      "0       0  0.7941  9.0809        54  470702  8523065625\n",
      "1       1  5.9567  4.7968        13  186555  1757726713\n",
      "2       2  8.3078  7.0407        74  322648  1137537235\n",
      "3       3  7.3665  2.5165        65  704587  6567393236\n",
      "4       4  4.0961  1.1307        31  472130  7440663949\n",
      "5       5  3.8099  1.9586        75  178065  6289802927\n",
      "6       6  6.3336  4.3720        13  666829  9931249544\n",
      "7       7  5.7409  6.7697        85  369002  5662813655\n",
      "8       8  4.3114  6.9410         3  166384  8471780938\n",
      "9       9  6.3414  0.0758        65  400060  1253803156\n",
      "--------------------------------------------------------------------------------\n",
      "10.0 0.0\n",
      "10.0 0.0\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:56:47.599971Z",
     "start_time": "2024-12-01T12:56:46.513107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = data.query('x < 7.5 & x > 1.5 & y < 7.5 & y > 1.5')\n",
    "print(data.shape)\n",
    "print(data.head(10))\n",
    "time_values = pd.to_datetime(data['time'], unit='s')\n",
    "time_index = pd.DatetimeIndex(time_values)\n",
    "print(time_index)\n",
    "print(type(time_index))\n",
    "print('-' * 80)\n",
    "# print(time_index.day)\n",
    "data.insert(data.shape[1], 'day', time_index.day)\n",
    "data.insert(data.shape[1], 'hour', time_index.hour)\n",
    "data.insert(data.shape[1], 'weekday', time_index.weekday)\n",
    "print(data.head(10))"
   ],
   "id": "4108eacaaf853bb4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10588792, 6)\n",
      "    row_id       x       y  accuracy    time    place_id\n",
      "1        1  5.9567  4.7968        13  186555  1757726713\n",
      "3        3  7.3665  2.5165        65  704587  6567393236\n",
      "5        5  3.8099  1.9586        75  178065  6289802927\n",
      "6        6  6.3336  4.3720        13  666829  9931249544\n",
      "7        7  5.7409  6.7697        85  369002  5662813655\n",
      "8        8  4.3114  6.9410         3  166384  8471780938\n",
      "10      10  2.0173  4.8627         6   21353  8684462954\n",
      "14      14  6.1550  1.9774         8  325411  2272949794\n",
      "16      16  3.2494  3.2096        75  777982  2123587484\n",
      "19      19  4.2683  1.8238        70  411437  2778700985\n",
      "DatetimeIndex(['1970-01-03 03:49:15', '1970-01-09 03:43:07',\n",
      "               '1970-01-03 01:27:45', '1970-01-08 17:13:49',\n",
      "               '1970-01-05 06:30:02', '1970-01-02 22:13:04',\n",
      "               '1970-01-01 05:55:53', '1970-01-04 18:23:31',\n",
      "               '1970-01-10 00:06:22', '1970-01-05 18:17:17',\n",
      "               ...\n",
      "               '1970-01-07 00:07:21', '1970-01-05 16:54:09',\n",
      "               '1970-01-07 03:39:15', '1970-01-07 04:51:57',\n",
      "               '1970-01-07 02:44:50', '1970-01-06 22:33:58',\n",
      "               '1970-01-01 11:47:22', '1970-01-02 10:51:20',\n",
      "               '1970-01-09 12:55:58', '1970-01-02 04:34:02'],\n",
      "              dtype='datetime64[ns]', name='time', length=10588792, freq=None)\n",
      "<class 'pandas.core.indexes.datetimes.DatetimeIndex'>\n",
      "--------------------------------------------------------------------------------\n",
      "    row_id       x       y  accuracy    time    place_id  day  hour  weekday\n",
      "1        1  5.9567  4.7968        13  186555  1757726713    3     3        5\n",
      "3        3  7.3665  2.5165        65  704587  6567393236    9     3        4\n",
      "5        5  3.8099  1.9586        75  178065  6289802927    3     1        5\n",
      "6        6  6.3336  4.3720        13  666829  9931249544    8    17        3\n",
      "7        7  5.7409  6.7697        85  369002  5662813655    5     6        0\n",
      "8        8  4.3114  6.9410         3  166384  8471780938    2    22        4\n",
      "10      10  2.0173  4.8627         6   21353  8684462954    1     5        3\n",
      "14      14  6.1550  1.9774         8  325411  2272949794    4    18        6\n",
      "16      16  3.2494  3.2096        75  777982  2123587484   10     0        5\n",
      "19      19  4.2683  1.8238        70  411437  2778700985    5    18        0\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:56:47.697782Z",
     "start_time": "2024-12-01T12:56:47.604482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(data.head(5))\n",
    "data.drop('time', axis=1, inplace=True)\n",
    "print(data.head(5))"
   ],
   "id": "619a3183cb2266d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id       x       y  accuracy    time    place_id  day  hour  weekday\n",
      "1       1  5.9567  4.7968        13  186555  1757726713    3     3        5\n",
      "3       3  7.3665  2.5165        65  704587  6567393236    9     3        4\n",
      "5       5  3.8099  1.9586        75  178065  6289802927    3     1        5\n",
      "6       6  6.3336  4.3720        13  666829  9931249544    8    17        3\n",
      "7       7  5.7409  6.7697        85  369002  5662813655    5     6        0\n",
      "   row_id       x       y  accuracy    place_id  day  hour  weekday\n",
      "1       1  5.9567  4.7968        13  1757726713    3     3        5\n",
      "3       3  7.3665  2.5165        65  6567393236    9     3        4\n",
      "5       5  3.8099  1.9586        75  6289802927    3     1        5\n",
      "6       6  6.3336  4.3720        13  9931249544    8    17        3\n",
      "7       7  5.7409  6.7697        85  5662813655    5     6        0\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:56:48.021734Z",
     "start_time": "2024-12-01T12:56:47.869648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "place_count = data.groupby(data['place_id']).count()\n",
    "print('-' * 80)\n",
    "print(place_count.head(5))"
   ],
   "id": "29a1b1ef07aa4573",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "            row_id    x    y  accuracy  day  hour  weekday\n",
      "place_id                                                  \n",
      "1000015801      78   78   78        78   78    78       78\n",
      "1000017288      52   52   52        52   52    52       52\n",
      "1000025138       3    3    3         3    3     3        3\n",
      "1000052096     957  957  957       957  957   957      957\n",
      "1000063498       1    1    1         1    1     1        1\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:56:48.195843Z",
     "start_time": "2024-12-01T12:56:48.191924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "place_afterCount = place_count[place_count.row_id > 3].index\n",
    "print(place_afterCount.shape)"
   ],
   "id": "54373e5c284cf20b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54178,)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:56:48.667878Z",
     "start_time": "2024-12-01T12:56:48.367935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = data[data['place_id'].isin(place_afterCount)]\n",
    "print(data.shape)\n",
    "print(data.head(5))"
   ],
   "id": "e12dfef165d34f9c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10572175, 8)\n",
      "   row_id       x       y  accuracy    place_id  day  hour  weekday\n",
      "1       1  5.9567  4.7968        13  1757726713    3     3        5\n",
      "3       3  7.3665  2.5165        65  6567393236    9     3        4\n",
      "5       5  3.8099  1.9586        75  6289802927    3     1        5\n",
      "6       6  6.3336  4.3720        13  9931249544    8    17        3\n",
      "7       7  5.7409  6.7697        85  5662813655    5     6        0\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:56:48.911023Z",
     "start_time": "2024-12-01T12:56:48.840571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y = data['place_id']\n",
    "x = data.drop('place_id', axis=1)\n",
    "print(x.columns)"
   ],
   "id": "8f28866159acb4d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['row_id', 'x', 'y', 'accuracy', 'day', 'hour', 'weekday'], dtype='object')\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:56:49.149868Z",
     "start_time": "2024-12-01T12:56:49.083690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = x.drop('row_id', axis=1)\n",
    "print(x.shape)"
   ],
   "id": "ba6bcf99095375e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10572175, 6)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:56:50.500528Z",
     "start_time": "2024-12-01T12:56:49.323142Z"
    }
   },
   "cell_type": "code",
   "source": "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=1)",
   "id": "76e0ac7f7606a5f0",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-01T12:56:50.686852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "print(scaler.mean_, scaler.var_)\n",
    "x_test = scaler.transform(x_test)\n",
    "print(scaler.mean_, scaler.var_)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(x_train, y_train)\n",
    "\n",
    "y_predict = knn.predict(x_test)\n",
    "print(y_predict)\n",
    "print(knn.score(x_test, y_test))"
   ],
   "id": "d2d3d992b5070808",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.51868166  4.50277542 82.81290535  5.33270039 11.50432059  3.07791749] [2.99923108e+00 2.98493569e+00 1.32938210e+04 7.11574485e+00\n",
      " 4.76580451e+01 2.96947688e+00]\n",
      "[ 4.51868166  4.50277542 82.81290535  5.33270039 11.50432059  3.07791749] [2.99923108e+00 2.98493569e+00 1.32938210e+04 7.11574485e+00\n",
      " 4.76580451e+01 2.96947688e+00]\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param = {'n_neighbors': [3, 5, 8, 10, 13]}\n",
    "\n",
    "gd = GridSearchCV(knn, param, cv=5)\n",
    "gd.fit(x_train, y_train)\n",
    "print(gd.score(x_test, y_test))\n",
    "print('-' * 80)\n",
    "print(gd.best_score_)\n",
    "print('-' * 80)\n",
    "print(gd.best_estimator_)\n",
    "print('-' * 80)\n",
    "print(gd.cv_results_)"
   ],
   "id": "5efe92048c5fd0d3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
